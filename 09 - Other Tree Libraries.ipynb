{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from utils import plot_tree_boundaries\n",
    "\n",
    "features = ['age','acutephysiologyscore']\n",
    "outcome = 'actualhospitalmortality'\n",
    "\n",
    "data = pd.read_csv('eicu_processed.csv')\n",
    "\n",
    "x = data[features]\n",
    "y = data[outcome]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM and xgboost\n",
    "\n",
    "In all of our previous workbooks, we've focused on creating decision trees that use only two features - `age` and `acutephysiologyscore`. While this made it easier to understand how each of the techniques we used results in different models, as we saw in Workbook 08 it doesn't necessarily provide us with the best quality model.\n",
    "\n",
    "Furthermore, so far we've focused on using the `sklearn` library to train our models. Whilst this library is extremely useful - and in many cases is able to create high-performing models - it is always helpful to have other tools in our toolbox. [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) is a framework for creating gradient boosted decision trees with a Python API. [xgboost](https://xgboost.readthedocs.io/en/stable/) is a similar library. Although they can both be used for the same end goal, they each have their own pros and cons.\n",
    "\n",
    "**Question:** What are the differences between LightGBM and xgboost? When might you want to use one over the other?\n",
    "\n",
    "This is an open-ended workbook - the purpose is for you to choose one of either LightGBM or xgboost and use it to create the best performing model for mortality prediction on our dataset that you can. Be sure to use your chosen library's developer documentation to help you, and feel free to use any other resources that you wish; if you want to go the extra mile, perhaps look at methods for [data imputation](https://scikit-learn.org/stable/modules/impute.html), or techniques to handle [imbalanced data](https://imbalanced-learn.org/stable/). Use the techniques from Workbook 08 to evaluate your final models - and be sure to remember what we learned in Workbook 03 - we don't want our models to overfit!\n",
    "\n",
    "**Tip:** If you want to install a new Python package, you can do so in a Jupyter Notebook code cell with the following command: `!pip install package-name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /Users/seanwang/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /Users/seanwang/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.11.4)\n",
      "Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f670ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create model instance\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(x_train, y_train)\n",
    "# make predictions\n",
    "preds = bst.predict(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Ideas\n",
    "\n",
    "As you may have noticed, decision trees have a lot of hyperparameters (e.g. `max_depth`) that we can change. One of the advantages of decision trees is that, quite often, these don't need any tuning and work well straight out of the box. However, it is possible to perform hyperparamter tuning to find the best model possible. You could take a look at libraries such as [optuna](https://optuna.readthedocs.io/en/stable/) that do this for you. Just be careful you don't overfit!\n",
    "\n",
    "Another advantage of decision trees is that they're highlhy interpretable - i.e., we can see which features are contributing most to the model's output. You could use a library such as [shap](https://github.com/shap/shap) to investigate which features are most important for your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
